% -----------------------------------------------------------------------
% File: CogSci_Template.bib
% -----------------------------------------------------------------------

% Modified : Eli M. Silk (esilk at pitt.edu)            05/24/2005
% Modified : David Noelle (dnoelle at ucmerced.edu)     11/19/2014


% ---- REMOVE EVERYTHING ABOVE ------ 

@article{zhang2018semantic,
  title={Semantic similarity to known second language words impacts learning of new meanings},
  author={Zhang, Yuanyue and Chen, Baoguo and Tang, Yixin and Yao, Panpan and Lu, Yao},
  journal={Frontiers in psychology},
  volume={9},
  pages={2048},
  year={2018},
  publisher={Frontiers}
}

@article{de2000hard,
  title={What is hard to learn is easy to forget: The roles of word concreteness, cognate status, and word frequency in foreign-language vocabulary learning and forgetting},
  author={De Groot, Annette MB and Keijzer, Rineke},
  journal={Language Learning},
  volume={50},
  number={1},
  pages={1--56},
  year={2000},
  publisher={Wiley Online Library}
}

@article{hill2015simlex,
  title={Simlex-999: Evaluating semantic models with (genuine) similarity estimation},
  author={Hill, Felix and Reichart, Roi and Korhonen, Anna},
  journal={Computational Linguistics},
  volume={41},
  number={4},
  pages={665--695},
  year={2015},
  publisher={MIT Press}
}

@article{rubenstein1965contextual,
  title={Contextual correlates of synonymy},
  author={Rubenstein, Herbert and Goodenough, John B},
  journal={Communications of the ACM},
  volume={8},
  number={10},
  pages={627--633},
  year={1965},
  publisher={ACM}
}

@inproceedings{bruni2012distributional,
  title={Distributional semantics in technicolor},
  author={Bruni, Elia and Boleda, Gemma and Baroni, Marco and Tran, Nam-Khanh},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
  pages={136--145},
  year={2012},
  organization={Association for Computational Linguistics}
}

@article{finkelstein2002placing,
  title={Placing search in context: The concept revisited},
  author={Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  journal={ACM Transactions on information systems},
  volume={20},
  number={1},
  pages={116--131},
  year={2002}
}

@article{chen2017evaluating,
  title={Evaluating vector-space models of analogy},
  author={Chen, Dawn and Peterson, Joshua C and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1705.04416},
  year={2017}
}

@article{gilquin2011efl,
  title={From EFL to ESL: evidence from the International Corpus of Learner English},
  author={Gilquin, Ga{\"e}tanelle and Granger, Sylviane and others},
  year = {2011}
}

@article{futagi2008computational,
  title={A computational approach to detecting collocation errors in the writing of non-native speakers of English},
  author={Futagi, Yoko and Deane, Paul and Chodorow, Martin and Tetreault, Joel},
  journal={Computer Assisted Language Learning},
  volume={21},
  number={4},
  pages={353--367},
  year={2008},
  publisher={Taylor \& Francis}
}

@inproceedings{Rozovskaya:2010:GCS:1870658.1870752,
 author = {Rozovskaya, Alla and Roth, Dan},
 title = {Generating Confusion Sets for Context-sensitive Error Correction},
 booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
 series = {EMNLP '10},
 year = {2010},
 location = {Cambridge, Massachusetts},
 pages = {961--970},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=1870658.1870752},
 acmid = {1870752},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@inproceedings{Rozovskaya:2011:ASM:2002472.2002589,
 author = {Rozovskaya, Alla and Roth, Dan},
 title = {Algorithm Selection and Model Adaptation for ESL Correction Tasks},
 booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
 series = {HLT '11},
 year = {2011},
 isbn = {978-1-932432-87-9},
 location = {Portland, Oregon},
 pages = {924--933},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=2002472.2002589},
 acmid = {2002589},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@inproceedings{ng2014conll,
  title={The CoNLL-2014 shared task on grammatical error correction},
  author={Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
  booktitle={Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task},
  pages={1--14},
  year={2014}
}

@article{chang2008automatic,
  title={An automatic collocation writing assistant for Taiwanese EFL learners: A case of corpus-based NLP technology},
  author={Chang, Yu-Chia and Chang, Jason S and Chen, Hao-Jan and Liou, Hsien-Chin},
  journal={Computer Assisted Language Learning},
  volume={21},
  number={3},
  pages={283--299},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{futagi2008computational,
  title={A computational approach to detecting collocation errors in the writing of non-native speakers of English},
  author={Futagi, Yoko and Deane, Paul and Chodorow, Martin and Tetreault, Joel},
  journal={Computer Assisted Language Learning},
  volume={21},
  number={4},
  pages={353--367},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{degani2010ambiguous,
  title={Ambiguous words are harder to learn},
  author={Degani, Tamar and Tokowicz, Natasha},
  journal={Bilingualism: Language and Cognition},
  volume={13},
  number={3},
  pages={299--314},
  year={2010},
  publisher={Cambridge University Press}
}

@article{bracken2017translation,
  title={Translation semantic variability: How semantic relatedness affects learning of translation-ambiguous words},
  author={Bracken, Jennifer and Degani, Tamar and Eddington, Chelsea and Tokowicz, Natasha},
  journal={Bilingualism: Language and Cognition},
  volume={20},
  number={4},
  pages={783--794},
  year={2017},
  publisher={Cambridge University Press}
}

@inproceedings{hopmanPredictorsL2Word,
  title = {Predictors of {{L2}} Word Learning Accuracy: {{A}} Big Data Investigation},
  abstract = {What makes some words harder to learn than others in a second language? Although some robust factors have been identified based on small scale experimental studies, many relevant factors are difficult to study in such experiments due to the amount of data necessary to test them. Here, we investigate what factors affect the ease of learning of a word in a second language using a large data set of users learning English as a second language through the Duolingo mobile app. In a regression analysis, we test and confirm the well-studied effect of cognate status on word learning accuracy. Furthermore, we find significant effects for both cross-linguistic semantic alignment and English semantic density, two novel predictors derived from large scale distributional models of lexical semantics. Finally, we provide data on several other psycholinguistically plausible word level predictors. We conclude with a discussion of the limits, benefits and future research potential of using big data for investigating second language learning.},
  language = {en},
  author = {Hopman, Elise W M and Thompson, Bill and Austerweil, Joseph L and Lupyan, Gary},
  keywords = {CogSci 2019 Idea},
  pages = {6},
  year = {2018},
  file = {/home/kanishka/Zotero/storage/M5WXILT2/Hopman et al. - Predictors of L2 word learning accuracy A big dat.pdf}
}

@article{yannakoudakisNewDatasetMethod2011,
  title = {A {{New Dataset}} and {{Method}} for {{Automatically Grading ESOL Texts}}},
  abstract = {We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of `English as a Second or Other Language' (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of `outlier' texts, we test the validity of our model and identify cases where the model's scores diverge from that of a human examiner.},
  language = {en},
  author = {Yannakoudakis, Helen and Briscoe, Ted and Medlock, Ben},
  year = {2011},
  keywords = {CogSci 2019 Idea},
  pages = {10},
  file = {/home/kanishka/Zotero/storage/D2X7BL8Y/Yannakoudakis et al. - A New Dataset and Method for Automatically Grading.pdf}
}

@article{thompsonQuantifyingSemanticAlignment,
  title = {Quantifying {{Semantic Alignment Across Languages}}},
  abstract = {Do all languages convey semantic knowledge in the same way? If language simply mirrors the structure of the world, the answer should be a qualified ``yes''. If, however, languages impose structure as much as reflecting it, then even ostensibly the ``same'' word in different languages may mean quite different things. We provide a first pass at a large-scale quantification of cross-linguistic semantic alignment of approximately 1000 meanings in 55 languages. We find that the translation equivalents in some domains (e.g., Time, Quantity, and Kinship) exhibit high alignment across languages while the structure of other domains (e.g., Politics, Food, Emotions, and Animals) exhibits substantial crosslinguistic variability. Our measure of semantic alignment correlates with known phylogenetic distances between languages: more phylogenetically distant languages have less semantic alignment. We also find semantic alignment to correlate with cultural distances between societies speaking the languages, suggesting a rich co-adaptation of language and culture even in domains of experience that appear most constrained by the natural world.},
  language = {en},
  author = {Thompson, Bill and Roberts, Sean and Roberts, Sean and Lupyan, Gary},
  pages = {6},
  file = {/home/kanishka/Zotero/storage/PJVT42G7/Thompson et al. - Quantifying Semantic Alignment Across Languages.pdf}
}

@inproceedings{kochmarCapturingAnomaliesChoice2013,
  address = {Hissar, Bulgaria},
  title = {Capturing {{Anomalies}} in the {{Choice}} of {{Content Words}} in {{Compositional Distributional Semantic Space}}},
  booktitle = {Proceedings of the {{International Conference Recent Advances}} in {{Natural Language Processing RANLP}} 2013},
  publisher = {{INCOMA Ltd. Shoumen, BULGARIA}},
  author = {Kochmar, Ekaterina and Briscoe, Ted},
  month = sep,
  year = {2013},
  pages = {365--372},
  file = {/home/kanishka/Zotero/storage/MVYTFRHG/Kochmar and Briscoe - 2013 - Capturing Anomalies in the Choice of Content Words.pdf}
}

@inproceedings{kochmarCrossLingualLexicoSemanticTransfer2016,
  address = {Berlin, Germany},
  title = {Cross-{{Lingual Lexico}}-{{Semantic Transfer}} in {{Language Learning}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Kochmar, Ekaterina and Shutova, Ekaterina},
  month = aug,
  year = {2016},
  pages = {974--983},
  file = {/home/kanishka/Zotero/storage/V5WZ7PQU/Kochmar and Shutova - 2016 - Cross-Lingual Lexico-Semantic Transfer in Language.pdf}
}

@inproceedings{kochmarModellingSemanticAcquisition2017,
  address = {Copenhagen, Denmark},
  title = {Modelling Semantic Acquisition in Second Language Learning},
  abstract = {Using methods of statistical analysis, we investigate how semantic knowledge is acquired in English as a second language and evaluate the pace of development across a number of predicate types and content word combinations, as well as across the levels of language proficiency and native languages. Our exploratory study helps identify the most problematic areas for language learners with different backgrounds and at different stages of learning.},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Kochmar, Ekaterina and Shutova, Ekaterina},
  month = sep,
  year = {2017},
  pages = {293--302},
  file = {/home/kanishka/Zotero/storage/H6FUNN4D/Kochmar and Shutova - 2017 - Modelling semantic acquisition in second language .pdf}
}

@article{kochmarDetectingLearnerErrors,
  title = {Detecting {{Learner Errors}} in the {{Choice}} of {{Content Words Using Compositional Distributional Semantics}}},
  abstract = {We describe a novel approach to error detection in adjective\textendash{}noun combinations. We present and release a new dataset of annotated errors where the examples are extracted from learner texts and annotated with error types. We show how compositional distributional semantic approaches can be applied to discriminate between correct and incorrect word combinations from learner data. Finally, we show how the output of the compositional distributional semantic models can be used as features in a classifier yielding good precision and accuracy.},
  language = {en},
  author = {Kochmar, Ekaterina and Briscoe, Ted},
  pages = {12},
  file = {/home/kanishka/Zotero/storage/262XZ9ZP/Kochmar and Briscoe - Detecting Learner Errors in the Choice of Content .pdf}
}

@article{nichollsCambridgeLearnerCorpus,
  title = {The {{Cambridge Learner Corpus}} - Error Coding and Analysis for Lexicography and {{ELT}}},
  abstract = {The Cambridge Learner Corpus is a 16 million-word corpus of Learner English collected by Cambridge University Press in collaboration with the University of Cambridge Local Examinations Syndicate (now Cambridge ESOL). It comprises English examination scripts, transcribed retaining all errors, written by learners of English with 86 different mother tongues. The scripts range across 8 EFL examinations and cover both general and business English. A 6 million-word component of the corpus has been error coded to date, using an error-coding system devised at CUP specifically for the Cambridge Learner Corpus. The majority of codes are based on a two-letter system in which the first letter represents the general type of error (e.g. wrong form, omission), while the second letter identifies the word class of the required word. There are 88 possible codes in all.},
  language = {en},
  year = {2003},
  author = {Nicholls, Diane},
  pages = {10},
  file = {/home/kanishka/Zotero/storage/BTHV74TM/Nicholls - The Cambridge Learner Corpus - error coding and an.pdf}
}

@inproceedings{kochmarUsingLearnerData2015,
  address = {Denver, Colorado},
  title = {Using {{Learner Data}} to {{Improve Error Correction}} in {{Adjective}}\textendash{{Noun Combinations}}},
  booktitle = {Proceedings of the {{Tenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Kochmar, Ekaterina and Briscoe, Ted},
  month = jun,
  year = {2015},
  pages = {233--242},
  file = {/home/kanishka/Zotero/storage/K8Y9RNAT/Kochmar and Briscoe - 2015 - Using Learner Data to Improve Error Correction in .pdf}
}

@inproceedings{dahlmeier2011correcting,
  title={Correcting semantic collocation errors with L1-induced paraphrases},
  author={Dahlmeier, Daniel and Ng, Hwee Tou},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  pages={107--117},
  year={2011},
  organization={Association for Computational Linguistics}
}

@article{eddingtonHowMeaningSimilarity2015,
  title = {How Meaning Similarity Influences Ambiguous Word Processing: The Current State of the Literature},
  volume = {22},
  issn = {1069-9384, 1531-5320},
  shorttitle = {How Meaning Similarity Influences Ambiguous Word Processing},
  doi = {10.3758/s13423-014-0665-7},
  language = {en},
  number = {1},
  journal = {Psychonomic Bulletin \& Review},
  author = {Eddington, Chelsea M. and Tokowicz, Natasha},
  month = feb,
  year = {2015},
  pages = {13-37},
  file = {/home/kanishka/Zotero/storage/DVVMIAPC/Eddington and Tokowicz - 2015 - How meaning similarity influences ambiguous word p.pdf}
}

@article{joulinBagTricksEfficient2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.01759},
  primaryClass = {cs},
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde{}CPU, and classify half a million sentences among\textasciitilde{}312K classes in less than a minute.},
  journal = {arXiv:1607.01759 [cs]},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  month = jul,
  year = {2016},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hemanthdevarapalli/Zotero/storage/T5YH8RKC/Joulin et al_2016_Bag of Tricks for Efficient Text Classification.pdf;/Users/hemanthdevarapalli/Zotero/storage/XU3QYXQI/1607.html}
}

@article{bojanowskiEnrichingWordVectors2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.04606},
  primaryClass = {cs},
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  journal = {arXiv:1607.04606 [cs]},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  month = jul,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Computer Science - Computation and Language},
  file = {/Users/hemanthdevarapalli/Zotero/storage/YARPN4SY/Bojanowski et al_2016_Enriching Word Vectors with Subword Information.pdf;/Users/hemanthdevarapalli/Zotero/storage/QJEVEETD/1607.html}
}

@article{al-rfouPolyglotDistributedWord2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.1662},
  primaryClass = {cs},
  title = {Polyglot: {{Distributed Word Representations}} for {{Multilingual NLP}}},
  shorttitle = {Polyglot},
  abstract = {Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.},
  journal = {arXiv:1307.1662 [cs]},
  author = {{Al-Rfou}, Rami and Perozzi, Bryan and Skiena, Steven},
  month = jul,
  year = {2013},
  keywords = {Computer Science - Machine Learning,Computer Science - Computation and Language},
  file = {/Users/hemanthdevarapalli/Zotero/storage/QA4ESJT9/Al-Rfou et al_2013_Polyglot.pdf;/Users/hemanthdevarapalli/Zotero/storage/YFUANXWX/1307.html}
}

@article{mikolovEfficientEstimationWord2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  journal = {arXiv:1301.3781 [cs]},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hemanthdevarapalli/Zotero/storage/IGYBHQ42/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf;/Users/hemanthdevarapalli/Zotero/storage/GSW4MM3M/1301.html}
}

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}


% -----------------------------------------------------------------------
% Document End
% -----------------------------------------------------------------------
